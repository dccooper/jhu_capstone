library(plotly)
data("iris")
plot_ly(iris, x = "Petal.Width", y = "Petal.Length", mode = "markers",
color = as.factor("Species"))
library(shiny)
library(plotly)
plot_ly(iris, x = "Petal.Width", y = "Petal.Length",
mode = "markers", color = as.factor("Species"))
data("airmiles")
data("airmiles")
data("iris")
library(plotly)
plot_ly(iris, x = iris(Petal.width), y = "Petal.Length",
mode = "markers", color = as.factor("Species"))
library(plotly)
plot_ly(x = time(airmiles), y = airmiles)
library(plotly)
plot_ly(iris, x = ~Petal.Width, y = ~Petal.Length,
mode = "markers", color = as.factor(~Species))
library(plotly)
plot_ly(iris, x = ~Petal.Width, y = ~Petal.Length,
mode = "markers", color = as.factor(Species))
library(plotly)
plot_ly(iris, x = ~Petal.Width, y = ~Petal.Length,
mode = "markers", color = as.factor("Species"))
library(plotly)
plot_ly(iris, x = ~Petal.Width, y = ~Petal.Length,
mode = "markers", color = ~Species)
library(plotly)
plot_ly(iris, x = ~Petal.Width, y = ~Petal.Length,
mode = "markers", color = ~Species)
install.packages(c("assertthat", "caret", "curl", "formatR", "htmltools", "jsonlite", "lme4", "markdown", "memoise", "plotly", "psych", "quantmod", "quantreg", "RcppArmadillo", "RcppEigen", "readxl", "rmarkdown", "rpart.plot", "rstan", "shiny", "sourcetools", "SparseM", "StanHeaders", "stringi", "tidyr", "tseries", "zoo"))
install.packages(c("assertthat", "caret", "curl", "formatR", "htmltools", "jsonlite", "lme4", "markdown", "memoise", "plotly", "psych", "quantmod", "quantreg", "RcppArmadillo", "RcppEigen", "readxl", "rmarkdown", "rpart.plot", "rstan", "shiny", "sourcetools", "SparseM", "StanHeaders", "stringi", "tidyr", "tseries", "zoo"))
install.packages(c("assertthat", "caret", "curl", "formatR", "htmltools", "jsonlite", "lme4", "markdown", "memoise", "plotly", "psych", "quantmod", "quantreg", "RcppArmadillo", "RcppEigen", "readxl", "rmarkdown", "rpart.plot", "rstan", "shiny", "sourcetools", "SparseM", "StanHeaders", "stringi", "tidyr", "tseries", "zoo"))
install.packages("tm")
install.packages("tidytext")
knitr::opts_chunk$set(echo = TRUE)
library(tidytext)
library(tm)
library(stringi)
# Reading files into R
en_twitter <- readLines("data/final/en_US/en_US.twitter.txt", encoding = "UTF-8", skipNul = TRUE)
en_news <- readLines("data/final/en_US/en_US.news.txt", encoding = "UTF-8", skipNul = TRUE)
en_blogs <- readLines("data/final/en_US/en_US.blogs.txt", encoding = "UTF-8", skipNul = TRUE)
# Get sizes of the original files
en_blogs_size <- file.info("data/final/en_US/en_US.blogs.txt")$size / 1024 ^ 2
en_news_size <- file.info("data/final/en_US/en_US.news.txt")$size / 1024 ^ 2
en_twitter_size <- file.info("data/final/en_US/en_US.twitter.txt")$size / 1024 ^ 2
# Get a count of the words in each file using Stringi library
en_blogs_wdcount <- stri_count_words(en_blogs)
en_news_wdcount <- stri_count_words(en_news)
en_twitter_wdcount <- stri_count_words(en_twitter)
# Build a data.table with a summary of all the data
corpora_summary <- data.frame(source = c("en_blogs", "en_news", "en_twitter"),
size_MB = c(en_blogs_size, en_news_size, en_twitter_size),
lines = c(length(en_blogs), length(en_twitter), length(en_news)),
words = c(sum(en_blogs_wdcount), sum(en_news_wdcount), sum(en_twitter_wdcount)),
mean_words = c(mean(en_blogs_wdcount), mean(en_news_wdcount), mean(en_twitter_wdcount)))
set.seed(1234)
corpus_sample <- c(sample(en_blogs, 1000),
sample(en_news, 1000),
sample(en_twitter, 1000))
corpus_train <- VCorpus(VectorSource(corpus_sample))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
corpus_train <- tm_map(corpus_train, toSpace, "(f|ht)tp(s?)://(.*)[.][a-z]+")
corpus_train <- tm_map(corpus_train, toSpace, "@[^\\s]+")
corpus_train <- tm_map(corpus_train, tolower)
corpus_train <- tm_map(corpus_train, removeWords, stopwords("english"))
corpus_train <- tm_map(corpus_train, removePunctuation)
corpus_train <- tm_map(corpus_train, removeNumbers)
corpus_train <- tm_map(corpus_train, stripWhitespace)
corpus_train <- tm_map(corpus_train, PlainTextDocument)
rm(en_blogs, en_twitter, en_news)
library(RWeka)
library(ggplot2)
bigram <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
trigram <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
getFreq <- function(tdm) {
freq <- sort(rowSums(as.matrix(tdm)), decreasing = TRUE)
return(data.frame(word = names(freq), freq = freq))
}
# Set standard for graphs
makePlot <- function(data, label) {
ggplot(data[1:10,], aes(reorder(word, freq), freq)) +
labs(x = label, y = "Frequency") +
theme_minimal() + geom_col() + xlab(NULL) +  coord_flip()
}
# n-grams analysis of data sample
corpus_unigram <- getFreq(removeSparseTerms(TermDocumentMatrix(corpus_train), 0.9999))
corpus_bigram <- getFreq(removeSparseTerms(TermDocumentMatrix(corpus_train, control = list(tokenize = bigram)), 0.9999))
corpus_trigram <- getFreq(removeSparseTerms(TermDocumentMatrix(corpus_train, control = list(tokenize = trigram)), 0.9999))
save(list = c("corpus_unigram", "corpus_trigram", "corpus_bigram"), file="data/ngrams.RData")
setwd("~/projects/jhu_capstone")
save(list = c("corpus_unigram", "corpus_trigram", "corpus_bigram"), file="data/ngrams.RData")
knitr::opts_chunk$set(echo = TRUE)
load("data/ngrams.RData")
library(tidytext)
library(tm)
library(stringi)
print(dfCount)
SAMPLES_DIR = "../Samples/"
#TAU_PACKAGE = "tau"
cFiles = c("blogs", "news", "twitter")
cTexts = c("case of cheese", "case of pretzels", "case of beer", "case of soda",
"mean the world", "mean the universe", "mean the most", "mean the best",
"make me the saddest", "make me the happiest", "make me the smelliest", "make me the bluest")
cTexts = c("struggling but the crowd", "struggling but the defense", "struggling but the players", "struggling but the referees",
"at the beach", "at the mall", "at the movies", "at the grocery",
"on my motorcycle", "on my phone", "on my horse", "on my way",
"quite some time", "quite some years", "quite some weeks", "quite some thing")
cTexts = c("but the crowd", "but the defense", "but the players", "but the referees",
"his little eyes", "his little toes", "his little fingers", "his little ears",
"during the bad", "during the hard", "during the sad", "during the worse",
"must be asleep", "must be insensitive", "must be insane", "must be callous")
#to initial tests
#cTexts = c("third meetin", "lot of rationalization", "never did no wanderin", "xptow233", "also", "i have")
# data frame to count frequency of texts searching
dfCount = data.frame(cTexts)
colnames(dfCount)[1] <- "Search_Texts"
dfCount$Count <- 0
nTxt <- 0		# points to a line in data frame 'dfCount'
COUNT = 2		# points to the column in 'dfCount' that counts frequency of texts found
search_files <- function(f, str)
{
txtFile <- paste0(SAMPLES_DIR,f,".txt")
print(paste("reading:",txtFile))
arq <- readLines(txtFile)
for (i in 1:length(arq) ) {
df <- data.frame(text=arq, stringsAsFactors=F)
}
nLines <- length(df$text)
for (i in 1:nLines ) {
nTxt <<- 0
lapply(cTexts, function(str){ search_text(str,df$text[i]) })
}
remove(df)
}
search_text <- function(strSearch, str_in) {
nTxt <<- nTxt + 1
if(grepl(strSearch, str_in))
dfCount[nTxt,COUNT] <<- dfCount[nTxt,COUNT] + 1
}
search_texts <- function(str)
{
lapply(cFiles, function(f) { search_files(f,str) })
print(paste("Couting occurrences of:",str,"=",n))
}
lapply(cFiles, function(f){ search_files(f) })
print(dfCount)
SAMPLES_DIR = "../Data/"
#TAU_PACKAGE = "tau"
cFiles = c("blogs", "news", "twitter")
cTexts = c("case of cheese", "case of pretzels", "case of beer", "case of soda",
"mean the world", "mean the universe", "mean the most", "mean the best",
"make me the saddest", "make me the happiest", "make me the smelliest", "make me the bluest")
cTexts = c("struggling but the crowd", "struggling but the defense", "struggling but the players", "struggling but the referees",
"at the beach", "at the mall", "at the movies", "at the grocery",
"on my motorcycle", "on my phone", "on my horse", "on my way",
"quite some time", "quite some years", "quite some weeks", "quite some thing")
cTexts = c("quite some years", "quite some time", "quite some weeks", "quite some thing",
"his little eyes", "his little toes", "his little fingers", "his little ears",
"during the bad", "during the hard", "during the sad", "during the worse",
"must be asleep", "must be insensitive", "must be insane", "must be callous")
#to initial tests
#cTexts = c("third meetin", "lot of rationalization", "never did no wanderin", "xptow233", "also", "i have")
# data frame to count frequency of texts searching
dfCount = data.frame(cTexts)
colnames(dfCount)[1] <- "Search_Texts"
dfCount$Count <- 0
nTxt <- 0		# points to a line in data frame 'dfCount'
COUNT = 2		# points to the column in 'dfCount' that counts frequency of texts found
search_files <- function(f, str)
{
txtFile <- paste0(SAMPLES_DIR,f,".txt")
print(paste("reading:",txtFile))
arq <- readLines(txtFile)
for (i in 1:length(arq) ) {
df <- data.frame(text=arq, stringsAsFactors=F)
}
nLines <- length(df$text)
for (i in 1:nLines ) {
nTxt <<- 0
lapply(cTexts, function(str){ search_text(str,df$text[i]) })
}
remove(df)
}
search_text <- function(strSearch, str_in) {
nTxt <<- nTxt + 1
if(grepl(strSearch, str_in))
dfCount[nTxt,COUNT] <<- dfCount[nTxt,COUNT] + 1
}
search_texts <- function(str)
{
lapply(cFiles, function(f) { search_files(f,str) })
print(paste("Couting occurrences of:",str,"=",n))
}
lapply(cFiles, function(f){ search_files(f) })
print(dfCount)
SAMPLES_DIR = "../Data/"
SAMPLES_DIR = "../Data/"
#TAU_PACKAGE = "tau"
cFiles = c("blogs", "news", "twitter")
cTexts = c("case of cheese", "case of pretzels", "case of beer", "case of soda",
"mean the world", "mean the universe", "mean the most", "mean the best",
"make me the saddest", "make me the happiest", "make me the smelliest", "make me the bluest")
cTexts = c("struggling but the crowd", "struggling but the defense", "struggling but the players", "struggling but the referees",
"at the beach", "at the mall", "at the movies", "at the grocery",
"on my motorcycle", "on my phone", "on my horse", "on my way",
"quite some time", "quite some years", "quite some weeks", "quite some thing")
cTexts = c("quite some years", "quite some time", "quite some weeks", "quite some thing",
"his little eyes", "his little toes", "his little fingers", "his little ears",
"during the bad", "during the hard", "during the sad", "during the worse",
"must be asleep", "must be insensitive", "must be insane", "must be callous")
#to initial tests
#cTexts = c("third meetin", "lot of rationalization", "never did no wanderin", "xptow233", "also", "i have")
# data frame to count frequency of texts searching
dfCount = data.frame(cTexts)
colnames(dfCount)[1] <- "Search_Texts"
dfCount$Count <- 0
nTxt <- 0		# points to a line in data frame 'dfCount'
COUNT = 2		# points to the column in 'dfCount' that counts frequency of texts found
search_files <- function(f, str)
{
txtFile <- paste0(SAMPLES_DIR,f,".txt")
print(paste("reading:",txtFile))
arq <- readLines(txtFile)
for (i in 1:length(arq) ) {
df <- data.frame(text=arq, stringsAsFactors=F)
}
nLines <- length(df$text)
for (i in 1:nLines ) {
nTxt <<- 0
lapply(cTexts, function(str){ search_text(str,df$text[i]) })
}
remove(df)
}
search_text <- function(strSearch, str_in) {
nTxt <<- nTxt + 1
if(grepl(strSearch, str_in))
dfCount[nTxt,COUNT] <<- dfCount[nTxt,COUNT] + 1
}
search_texts <- function(str)
{
lapply(cFiles, function(f) { search_files(f,str) })
print(paste("Couting occurrences of:",str,"=",n))
}
lapply(cFiles, function(f){ search_files(f) })
print(dfCount)
load("~/projects/jhu_capstone/data/sample_data.RData")
save(en_bl_sample, file="sample/blog.txt")
save(en_bl_sample, file="data/blog.txt")
load("~/projects/jhu_capstone/data/sample_data.RData")
save(en_bl_sample, file="data/blog.txt")
save(en_nw_sample, file="data/news.txt")
save(en_tw_sample, file="data/twitter.txt")
SAMPLES_DIR = "../Data/"
#TAU_PACKAGE = "tau"
cFiles = c("blogs", "news", "twitter")
cTexts = c("case of cheese", "case of pretzels", "case of beer", "case of soda",
"mean the world", "mean the universe", "mean the most", "mean the best",
"make me the saddest", "make me the happiest", "make me the smelliest", "make me the bluest")
cTexts = c("struggling but the crowd", "struggling but the defense", "struggling but the players", "struggling but the referees",
"at the beach", "at the mall", "at the movies", "at the grocery",
"on my motorcycle", "on my phone", "on my horse", "on my way",
"quite some time", "quite some years", "quite some weeks", "quite some thing")
cTexts = c("quite some years", "quite some time", "quite some weeks", "quite some thing",
"his little eyes", "his little toes", "his little fingers", "his little ears",
"during the bad", "during the hard", "during the sad", "during the worse",
"must be asleep", "must be insensitive", "must be insane", "must be callous")
#to initial tests
#cTexts = c("third meetin", "lot of rationalization", "never did no wanderin", "xptow233", "also", "i have")
# data frame to count frequency of texts searching
dfCount = data.frame(cTexts)
colnames(dfCount)[1] <- "Search_Texts"
dfCount$Count <- 0
nTxt <- 0		# points to a line in data frame 'dfCount'
COUNT = 2		# points to the column in 'dfCount' that counts frequency of texts found
search_files <- function(f, str)
{
txtFile <- paste0(SAMPLES_DIR,f,".txt")
print(paste("reading:",txtFile))
arq <- readLines(txtFile)
for (i in 1:length(arq) ) {
df <- data.frame(text=arq, stringsAsFactors=F)
}
nLines <- length(df$text)
for (i in 1:nLines ) {
nTxt <<- 0
lapply(cTexts, function(str){ search_text(str,df$text[i]) })
}
remove(df)
}
search_text <- function(strSearch, str_in) {
nTxt <<- nTxt + 1
if(grepl(strSearch, str_in))
dfCount[nTxt,COUNT] <<- dfCount[nTxt,COUNT] + 1
}
search_texts <- function(str)
{
lapply(cFiles, function(f) { search_files(f,str) })
print(paste("Couting occurrences of:",str,"=",n))
}
lapply(cFiles, function(f){ search_files(f) })
print(dfCount)
SAMPLES_DIR = "../data/"
#TAU_PACKAGE = "tau"
cFiles = c("blogs", "news", "twitter")
cTexts = c("case of cheese", "case of pretzels", "case of beer", "case of soda",
"mean the world", "mean the universe", "mean the most", "mean the best",
"make me the saddest", "make me the happiest", "make me the smelliest", "make me the bluest")
cTexts = c("struggling but the crowd", "struggling but the defense", "struggling but the players", "struggling but the referees",
"at the beach", "at the mall", "at the movies", "at the grocery",
"on my motorcycle", "on my phone", "on my horse", "on my way",
"quite some time", "quite some years", "quite some weeks", "quite some thing")
cTexts = c("quite some years", "quite some time", "quite some weeks", "quite some thing",
"his little eyes", "his little toes", "his little fingers", "his little ears",
"during the bad", "during the hard", "during the sad", "during the worse",
"must be asleep", "must be insensitive", "must be insane", "must be callous")
#to initial tests
#cTexts = c("third meetin", "lot of rationalization", "never did no wanderin", "xptow233", "also", "i have")
# data frame to count frequency of texts searching
dfCount = data.frame(cTexts)
colnames(dfCount)[1] <- "Search_Texts"
dfCount$Count <- 0
nTxt <- 0		# points to a line in data frame 'dfCount'
COUNT = 2		# points to the column in 'dfCount' that counts frequency of texts found
search_files <- function(f, str)
{
txtFile <- paste0(SAMPLES_DIR,f,".txt")
print(paste("reading:",txtFile))
arq <- readLines(txtFile)
for (i in 1:length(arq) ) {
df <- data.frame(text=arq, stringsAsFactors=F)
}
nLines <- length(df$text)
for (i in 1:nLines ) {
nTxt <<- 0
lapply(cTexts, function(str){ search_text(str,df$text[i]) })
}
remove(df)
}
search_text <- function(strSearch, str_in) {
nTxt <<- nTxt + 1
if(grepl(strSearch, str_in))
dfCount[nTxt,COUNT] <<- dfCount[nTxt,COUNT] + 1
}
search_texts <- function(str)
{
lapply(cFiles, function(f) { search_files(f,str) })
print(paste("Couting occurrences of:",str,"=",n))
}
lapply(cFiles, function(f){ search_files(f) })
print(dfCount)
write.table(en_bl_sm_tidy, file = "data/blogs.txt", sep = "\t", col.names = NA)
write.table(en_bl_sm_tidy[,2], file = "data/blogs.txt", sep = "\t", col.names = NA)
View(en_bl_sm_tidy)
write.table(en_bl_sm_tidy$word, file = "data/blogs.txt", sep = "\t", col.names = NA)
SAMPLES_DIR = "../data/"
#TAU_PACKAGE = "tau"
cFiles = c("blogs", "news", "twitter")
cTexts = c("case of cheese", "case of pretzels", "case of beer", "case of soda",
"mean the world", "mean the universe", "mean the most", "mean the best",
"make me the saddest", "make me the happiest", "make me the smelliest", "make me the bluest")
cTexts = c("struggling but the crowd", "struggling but the defense", "struggling but the players", "struggling but the referees",
"at the beach", "at the mall", "at the movies", "at the grocery",
"on my motorcycle", "on my phone", "on my horse", "on my way",
"quite some time", "quite some years", "quite some weeks", "quite some thing")
cTexts = c("quite some years", "quite some time", "quite some weeks", "quite some thing",
"his little eyes", "his little toes", "his little fingers", "his little ears",
"during the bad", "during the hard", "during the sad", "during the worse",
"must be asleep", "must be insensitive", "must be insane", "must be callous")
#to initial tests
#cTexts = c("third meetin", "lot of rationalization", "never did no wanderin", "xptow233", "also", "i have")
# data frame to count frequency of texts searching
dfCount = data.frame(cTexts)
colnames(dfCount)[1] <- "Search_Texts"
dfCount$Count <- 0
nTxt <- 0		# points to a line in data frame 'dfCount'
COUNT = 2		# points to the column in 'dfCount' that counts frequency of texts found
search_files <- function(f, str)
{
txtFile <- paste0(SAMPLES_DIR,f,".txt")
print(paste("reading:",txtFile))
arq <- readLines(txtFile)
for (i in 1:length(arq) ) {
df <- data.frame(text=arq, stringsAsFactors=F)
}
nLines <- length(df$text)
for (i in 1:nLines ) {
nTxt <<- 0
lapply(cTexts, function(str){ search_text(str,df$text[i]) })
}
remove(df)
}
search_text <- function(strSearch, str_in) {
nTxt <<- nTxt + 1
if(grepl(strSearch, str_in))
dfCount[nTxt,COUNT] <<- dfCount[nTxt,COUNT] + 1
}
search_texts <- function(str)
{
lapply(cFiles, function(f) { search_files(f,str) })
print(paste("Couting occurrences of:",str,"=",n))
}
lapply(cFiles, function(f){ search_files(f) })
print(dfCount)
SAMPLES_DIR = "data/"
#TAU_PACKAGE = "tau"
cFiles = c("blogs", "news", "twitter")
cTexts = c("case of cheese", "case of pretzels", "case of beer", "case of soda",
"mean the world", "mean the universe", "mean the most", "mean the best",
"make me the saddest", "make me the happiest", "make me the smelliest", "make me the bluest")
cTexts = c("struggling but the crowd", "struggling but the defense", "struggling but the players", "struggling but the referees",
"at the beach", "at the mall", "at the movies", "at the grocery",
"on my motorcycle", "on my phone", "on my horse", "on my way",
"quite some time", "quite some years", "quite some weeks", "quite some thing")
cTexts = c("quite some years", "quite some time", "quite some weeks", "quite some thing",
"his little eyes", "his little toes", "his little fingers", "his little ears",
"during the bad", "during the hard", "during the sad", "during the worse",
"must be asleep", "must be insensitive", "must be insane", "must be callous")
#to initial tests
#cTexts = c("third meetin", "lot of rationalization", "never did no wanderin", "xptow233", "also", "i have")
# data frame to count frequency of texts searching
dfCount = data.frame(cTexts)
colnames(dfCount)[1] <- "Search_Texts"
dfCount$Count <- 0
nTxt <- 0		# points to a line in data frame 'dfCount'
COUNT = 2		# points to the column in 'dfCount' that counts frequency of texts found
search_files <- function(f, str)
{
txtFile <- paste0(SAMPLES_DIR,f,".txt")
print(paste("reading:",txtFile))
arq <- readLines(txtFile)
for (i in 1:length(arq) ) {
df <- data.frame(text=arq, stringsAsFactors=F)
}
nLines <- length(df$text)
for (i in 1:nLines ) {
nTxt <<- 0
lapply(cTexts, function(str){ search_text(str,df$text[i]) })
}
remove(df)
}
search_text <- function(strSearch, str_in) {
nTxt <<- nTxt + 1
if(grepl(strSearch, str_in))
dfCount[nTxt,COUNT] <<- dfCount[nTxt,COUNT] + 1
}
search_texts <- function(str)
{
lapply(cFiles, function(f) { search_files(f,str) })
print(paste("Couting occurrences of:",str,"=",n))
}
lapply(cFiles, function(f){ search_files(f) })
print(dfCount)
export_blog <- en_bl_sm_tidy[, 2]
export_blog <- en_bl_sm_tidy$word
export_blog <- en_bl_sm_tidy$word
write.table(export_blog, file = "data/blogs.txt", sep = "\t", col.names = NA)
write.table(en_bl_sm_tidy$word, file = "data/blogs.txt", sep = "\t", col.names = NA, row.names = FALSE)
write.table(en_bl_sm_tidy$word, file = "data/blogs.txt", sep = "\t", col.names = TRUE, row.names = FALSE)
write.table(en_tw_sm_tidy$word, file = "data/twitter.txt", sep = "\t", col.names = TRUE, row.names = FALSE)
write.table(en_nw_sm_tidy$word, file = "data/news.txt", sep = "\t", col.names = TRUE, row.names = FALSE)
SAMPLES_DIR = "data/"
#TAU_PACKAGE = "tau"
cFiles = c("blogs", "news", "twitter")
cTexts = c("case of cheese", "case of pretzels", "case of beer", "case of soda",
"mean the world", "mean the universe", "mean the most", "mean the best",
"make me the saddest", "make me the happiest", "make me the smelliest", "make me the bluest")
cTexts = c("struggling but the crowd", "struggling but the defense", "struggling but the players", "struggling but the referees",
"at the beach", "at the mall", "at the movies", "at the grocery",
"on my motorcycle", "on my phone", "on my horse", "on my way",
"quite some time", "quite some years", "quite some weeks", "quite some thing")
cTexts = c("quite some years", "quite some time", "quite some weeks", "quite some thing",
"his little eyes", "his little toes", "his little fingers", "his little ears",
"during the bad", "during the hard", "during the sad", "during the worse",
"must be asleep", "must be insensitive", "must be insane", "must be callous")
#to initial tests
#cTexts = c("third meetin", "lot of rationalization", "never did no wanderin", "xptow233", "also", "i have")
# data frame to count frequency of texts searching
dfCount = data.frame(cTexts)
colnames(dfCount)[1] <- "Search_Texts"
dfCount$Count <- 0
nTxt <- 0		# points to a line in data frame 'dfCount'
COUNT = 2		# points to the column in 'dfCount' that counts frequency of texts found
search_files <- function(f, str)
{
txtFile <- paste0(SAMPLES_DIR,f,".txt")
print(paste("reading:",txtFile))
arq <- readLines(txtFile)
for (i in 1:length(arq) ) {
df <- data.frame(text=arq, stringsAsFactors=F)
}
nLines <- length(df$text)
for (i in 1:nLines ) {
nTxt <<- 0
lapply(cTexts, function(str){ search_text(str,df$text[i]) })
}
remove(df)
}
search_text <- function(strSearch, str_in) {
nTxt <<- nTxt + 1
if(grepl(strSearch, str_in))
dfCount[nTxt,COUNT] <<- dfCount[nTxt,COUNT] + 1
}
search_texts <- function(str)
{
lapply(cFiles, function(f) { search_files(f,str) })
print(paste("Couting occurrences of:",str,"=",n))
}
lapply(cFiles, function(f){ search_files(f) })
print(dfCount)
